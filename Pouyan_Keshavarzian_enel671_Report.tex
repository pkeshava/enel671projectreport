
%% bare_jrnl.tex
%% V1.4b
%% 2015/08/26
%% by Michael Shell
%% see http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8b or later) with an IEEE
%% journal paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/pkg/ieeetran
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE!
%% User assumes all risk.
%% In no event shall the IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%*************************************************************************


% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. The IEEE's font choices and paper sizes can   ***
% *** trigger bugs that do not appear when using other class files.       ***                          ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/



\documentclass[journal]{IEEEtran}
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[journal]{../sty/IEEEtran}





% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/pkg/ifpdf
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
%\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of the IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/pkg/cite
% The documentation is contained in the cite.sty file itself.






% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at:
% http://www.ctan.org/pkg/graphicx
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/pkg/epslatex
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex



\usepackage{amsmath,amsthm,amssymb}
\usepackage{hyperref}
\usepackage{float}    % Required to keep images where they are put
\usepackage{graphicx} % Required for the inclusion of images
\usepackage{listings} % Required for the inclusion of source code
\usepackage{mathtools}
\usepackage{enumitem} % Requred to use different characters for enumerated lists
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{subfigure}
\usepackage[]{mcode} % Matlab code
\providecommand{\e}[1]{\ensuremath{\times 10^{#1}}}
\usepackage[export]{adjustbox}
\usepackage{array}
\usepackage{caption}
\usepackage[backend=bibtex, firstinits=false, style=ieee]{biblatex}
\addbibresource{Bibliog.bib}
% *** MATH PACKAGES ***
%
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics.
%
% Note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/amsmath





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as the IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/pkg/algorithms
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/pkg/algorithmicx




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/array


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.




% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a larger sans serif font
% than the serif footnote size font used in traditional IEEE formatting
% and thus the need to invoke different subfig.sty package options depending
% on whether compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/pkg/subfig




% *** FLOAT PACKAGES ***
%
\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure.
% Be aware that LaTeX2e kernels dated 2015 and later have fixltx2e.sty's
% corrections already built into the system in which case a warning will
% be issued if an attempt is made to load fixltx2e.sty as it is no longer
% needed.
% The latest version and documentation can be found at:
% http://www.ctan.org/pkg/fixltx2e


\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/stfloats
% Do not use the stfloats baselinefloat ability as the IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that the IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as the IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/pkg/dblfloatfix




%\ifCLASSOPTIONcaptionsoff
%  \usepackage[nomarkers]{endfloat}
% \let\MYoriglatexcaption\caption
% \renewcommand{\caption}[2][\relax]{\MYoriglatexcaption[#2]{#2}}
%\fi
% endfloat.sty was written by James Darrell McCauley, Jeff Goldberg and
% Axel Sommerfeldt. This package may be useful when used in conjunction with
% IEEEtran.cls'  captionsoff option. Some IEEE journals/societies require that
% submissions have lists of figures/tables at the end of the paper and that
% figures/tables without any captions are placed on a page by themselves at
% the end of the document. If needed, the draftcls IEEEtran class option or
% \CLASSINPUTbaselinestretch interface can be used to increase the line
% spacing as well. Be sure and use the nomarkers option of endfloat to
% prevent endfloat from "marking" where the figures would have been placed
% in the text. The two hack lines of code above are a slight modification of
% that suggested by in the endfloat docs (section 8.4.1) to ensure that
% the full captions always appear in the list of figures/tables - even if
% the user used the short optional argument of \caption[]{}.
% IEEE papers do not typically make use of \caption[]'s optional argument,
% so this should not be an issue. A similar trick can be used to disable
% captions of packages such as subfig.sty that lack options to turn off
% the subcaptions:
% For subfig.sty:
% \let\MYorigsubfloat\subfloat
% \renewcommand{\subfloat}[2][\relax]{\MYorigsubfloat[]{#2}}
% However, the above trick will not work if both optional arguments of
% the \subfloat command are used. Furthermore, there needs to be a
% description of each subfigure *somewhere* and endfloat does not add
% subfigure captions to its list of figures. Thus, the best approach is to
% avoid the use of subfigure captions (many IEEE journals avoid them anyway)
% and instead reference/explain all the subfigures within the main caption.
% The latest version of endfloat.sty and its documentation can obtained at:
% http://www.ctan.org/pkg/endfloat
%
% The IEEEtran \ifCLASSOPTIONcaptionsoff conditional can also be used
% later in the document, say, to conditionally put the References on a
% page by themselves.




% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/url
% Basically, \url{my_url_here}.


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
\title{ENEL 671 Adpative Signal Processing\\ Project Report}
%
%
% author names and IEEE memberships
% note positions of commas and nonbreaking spaces ( ~ ) LaTeX will not break
% a structure at a ~ so this keeps an author's name from being broken across
% two lines.
% use \thanks{} to gain access to the first footnote area
% a separate \thanks must be used for each paragraph as LaTeX2e's \thanks
% was not built to handle multiple paragraphs
%

\author{Pouyan Keshavarzian,~\IEEEmembership{Student Member,~IEEE,}
\\ pkeshava@ucalgary.ca
\\ UCID 10053710
\\Electrical \& Computer Engineering, University of Calgary}

% The paper headers
\markboth{Pouyan Keshavarzian ENEL 671 FALL 2016}%
{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for IEEE Journals}
% The only time the second header will appear is for the odd numbered pages
% after the title page when using the twoside option.
%
% *** Note that you probably will NOT want to include the author's ***
% *** name in the headers of peer review papers.                   ***
% You can use \ifCLASSOPTIONpeerreview for conditional compilation here if
% you desire.


% make the title area
\maketitle
% As a general rule, do not put math, special symbols or citations
% in the abstract or keywords.
\begin{abstract}
The purpose of this project is to analyze the performance of three different
adaptive algorithms. These algorithms, which can be used for a variety of
applications such as echo cancellation and beamforming, are implemented for
the purposes of equalizing a dispersive channel. The first algorithm
investigated is the Least Mean Squares (LMS). The LMS filter uses a search method to minimize the mean square error.
The performance of the LMS is then compared to the Recursive Least Squares (RLS) method, which minimizes a weighted linear least squares cost funtion.
Finally the Recursive Least Squares Lattice (RLSL) algorithm is implemented in the third project for joint process estimation.
The algorithms are tested against multiple channels with varying eigenvalue spreads. Furthermore
different design parameters such as step size and filter order are evaluated.
The performance with regards to convergence speed and complexity is discussed. The RLS and RLSL appear to have
performance advantages in terms of convergence speed at the cost of increased complexity.
\end{abstract}

% Note that keywords are not normally used for peerreview papers.

% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle

\small

\section{Introduction}
% The very first letter is a 2 line initial drop letter followed
% by the rest of the first word in caps.
%
% form to use if the first word consists of a single letter:
% \IEEEPARstart{A}{demo} file is ....
%
% form to use if you need the single drop letter followed by
% normal text (unknown if ever used by the IEEE):
% \IEEEPARstart{A}{}demo file is ....
%
% Some journals put the first two words in caps:
% \IEEEPARstart{T}{his demo} file is ....
%
% Here we have the typical use of a "T" for an initial drop letter
% and "HIS" in caps to complete the first word.
\IEEEPARstart{A}{daptive} filtering is used for a variety of applications. When communicating
across a dispersive channel, a signal is destorted by effects of multipath, noise, etc. \cite{goldenberg1985digital}.
To recover the correct message at the receiver, an adaptive filter is implemented to equalize the channel. In general, an
algorithm is used to calculate the optimum filter coefficients to minimize the error between transmit and receive. In practice, a
signal transmitted along a channel contains a preamble, which is a known sequence of data that is used to esimate the parameters of the channel.
The statistics are assumed to be stationary in the window of time that each preamble is sent. \\ \indent The most common adaptive equalizers involve
the LMS, RLS, and RLSL algorithms. The performance of these methods varies and they each have their advantages and drawbacks.
The LMS is relatively simplistic in terms of hardware/software resources. However, it's drawback is that it uses a stochastic search gradient
which limits its ability to find an optimum solution, is inefficient and not practically capable of removing all distortions. The RLS algorithm is typically an order of magnitude
faster because it whitens the input data by using the inverse correlation matrix of the data, assumed to be of zero mean \cite{lecturenotes}. The RLSL
algorithm uses a lattice predictor along with a stage that estimates the error of the next filter order at each time step.\\
\indent The methodology used for designing of the filters in this project will be discussed. Afterwards a detailed analysis of the results
for each project and a summary of their implications will be discussed. Finally a conclusions section will highlight the general results drawn from each
experiment.



% You must have at least 2 lines in the paragraph with the drop letter
% (should never be an issue)


\section{Equalization Methodology}
\begin{figure*}
  \centering
  \captionsetup{justification=centering,font = small}
  \includegraphics[width=0.85\textwidth] {Plots/BlockDiagram}
  \caption{Block Diagram for Equalization \cite{lecturenotes}}
    \label{fig:BLOCK}
\end{figure*}
\IEEEPARstart{A}{daptive} equalization is designed to remove unwanted signals from
a channel output which can be caused by multipath or other distortions. In Figure~\ref{fig:BLOCK} the
general block diagram that is simulated in these experiments is outlined. A data source that
generates a random BPSK signal goes through a dispersive channel that has three paths and white noise.
The adaptive filter reduces the effects of these signal distortions to generate the output.
Before the specifics of this block diagram is discussed, a few general principles of
Wiener filter theory will be presented. The autocorrelation matrix is defined as
$$\boldsymbol{R} = E[\boldsymbol{u}(n)\boldsymbol{u}(n)^T] $$
\\
and the cross-correlation:
$$\boldsymbol{p} = E[\boldsymbol{u}(n)\boldsymbol{d}(n)]$$

it can be shown from \cite{haykin2008adaptive} that the Wiener-Hopf or Normal Equation can be solved for
the optimum tap weights:
 $$\boldsymbol{Rw} = \boldsymbol{p}$$
However, this is computationaly wasteful and the matrix \textbf{R} may not
always be invertable, therefore adaptive algorithms are used to determine the tap weight vector.
\[
\boldsymbol{w} =\begin{bmatrix}w_0 & w_1 & ... & w_{M-1}\end{bmatrix}^T\\\mbox{}
\]
The output of the channel, which is the input to the adaptive filter is defined as:
\[
\boldsymbol{u}(n) =\begin{bmatrix}u(n) & u(n-1) & ...  & u(n-M+1)\end{bmatrix}^T\\\mbox{}
\]
$$= \boldsymbol{h}^T(n)\boldsymbol{a}(n) + \boldsymbol{v}(n)$$
where  \textbf{h} is known as the channel impulse response. The impulse response for the four channels
with varying distortion are defined as
\begin{table}[H]
\centering
\captionsetup{font = small}
 \begin{tabular}{ | c || c | c | c |}
    \hline
    Channel & $\boldsymbol{h}_1$ & $\boldsymbol{h}_2$ & $\boldsymbol{h}_3$\\
    \hline\hline
    1 & 0.2194 & 1 & 0.2194 \\
    \hline
    2 & 0.2798 & 1 & 0.2798 \\
    \hline
    3 & 0.3365 & 1 & 0.3365\\
    \hline
    4 & 0.3887 & 1 & 0.3887\\
    \hline
  \end{tabular}
  \caption{Channel Parameters}
  \label{table:channel}
\end{table}
To compare the output of the filter to the truth data source, appropriate delay must be added to the BPSK data. The optimal delay is given by
half the filter order, that is:
$$\Delta = \dfrac{M+1}{2}$$
\section{Pre Experiment Calculations}
From analysis we are given that the input autocorrelation matrix has
a quintdiagonal structure of the form given by:\\
\[
\boldsymbol{R_{input}}=
    \begin{bsmallmatrix}
    r(0) & r(1) & r(2) & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\
    r(1) & r(0) & r(1) & r(2) & 0 & 0 & 0 & 0 & 0 & 0 & 0\\
    r(2) & r(1) & r(0) & r(1) & r(2) & 0 & 0 & 0 & 0 & 0 & 0\\
    0 & r(2) & r(1) & r(0) & r(1) & r(2) & 0 & 0 & 0 & 0 & 0\\
    0 & 0 & r(2) & r(1) & r(0) & r(1) & r(2) & 0 & 0 & 0 & 0\\
    0 & 0 & 0 & r(2) & r(1) & r(0) & r(1) & r(2) & 0 & 0 & 0\\
    0 & 0 & 0 & 0 & r(2) & r(1) & r(0) & r(1) & r(2) & 0 & 0\\
    0 & 0 & 0 & 0 & 0 & r(2) & r(1) & r(0) & r(1) & r(2) & 0\\
    0 & 0 & 0 & 0 & 0 & 0 & r(2) & r(1) & r(0) & r(1) & r(2)\\
    0 & 0 & 0 & 0 & 0 & 0 & 0 & r(2) & r(1) & r(0) & r(1)\\
    0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & r(2) & r(1) & r(0)\\
  \end{bsmallmatrix}
\]
\\
$$  r(0) = h_1^2 + h_2^2 + h_3^2, \qquad  r(1) = h_1h_2 + h_2h_3 $$
$$  r(2) = h_1h_3 $$
using the equations given the input autocorelation matrix and can be evaluated
for each channel.
The eigenvalues are calculated using the autocorrelation matrix with noise of $ \sigma^2 = 0.0001$.
The autocorrelation of the noise matrix is simply a diagonal matrix of the variance denoted by, $\boldsymbol{R}_v$ and the overall
autocorrelation with noise is defined as:
$$ \boldsymbol{R} = \boldsymbol{R}_{input} + \boldsymbol{R}_v$$

The values for comparison are shon in Tables ~\ref{table:eigenspreadn}
\linespread{0.8}
\begin{table}[H]
  \centering
  \captionsetup{font = small}
 \begin{tabular}{ | c || p{1.5cm} | p{1.5cm} | p{1.5cm} |}
    \hline
    Channel & Min Eigenvalue, $\lambda_{min}$  & Max Eigenvalue, $\lambda_{max}$ & Eigenvalue Spread $\chi = \lambda_{max}/\lambda_{min}$\\
    \hline\hline
    1 & 0.3330 & 2.2086 & 6.0915 \\
    \hline
    2 & 0.2127 & 2.3752 & 11.1663 \\
    \hline
    3 & 0.1246 & 2.7256 & 21.8725 \\
    \hline
    4 & 0.0647 & 3.0695 & 47.4274 \\
    \hline
  \end{tabular}
  \caption{Eigenvalues With Noise}
  \label{table:eigenspreadn}
\end{table}


As is shown, each channel has progressively worse eigenvalue spread. The effect
of this on certain algorithms will be discussed in the following sections

\section{Project 1: Least Mean Squares Algorithm}
The LMS algorithm implements a search method using a step size, $\mu$, to iteratively update the
filter tap weights to find a solution that minimizes the mean square error\cite{widrow1976stationary}.

The mean squared error is defined by:
$$\boldsymbol{J}(n) = \sigma^2_d - 2\boldsymbol{w}^T\boldsymbol{p} + \boldsymbol{w}^T\boldsymbol{R}\boldsymbol{w}$$ where
$\sigma^2_d$ is the deterministic variance and the tap-weight vector is updated using:
$$ \boldsymbol{w}(n)= \boldsymbol{w}(n-1)+\mu\boldsymbol{u}(n)e(n)$$
The filter output is the tap-weight vector multipled by the tap input vector:
$$\hat{d}(n) = y(n) = \boldsymbol{w}^T(n)\boldsymbol{u}(n) = \boldsymbol{u}^T(n)\boldsymbol{w}(n)$$
Intuitively the error of the filter is simply the estimate subtracted from the desired signal.
$$e(n) = d(n) - \hat{d}(n) = d(n) - \boldsymbol{w}^T(n)\boldsymbol{u}(n)$$
Which in the context of this project is the randomly generated BPSK signal, $a(n)$. The summary of the implemented LMS algorithm is as follows:\\
\\
\textbf{Step 1:} $\boldsymbol{w}(0) = \boldsymbol{0} $\\
\textbf{Step 2:}\\
For $k = 1:K$\\
and,\\
For $n = 1:N$\\
(i.e. for each value of the input sequence $u(n), n = 1,2,...N$)
\\ \indent form the tap-input vector $\boldsymbol{u}(n)$ and compute the filter output:
\begin{equation}
  y(n) = \boldsymbol{w}^T(n-1)\boldsymbol{u}(n)
\end{equation}
\indent \textbf{Step 3:} Compute the error:
\begin{equation}
  e(n) = d(n) - \hat{d}(n)
\end{equation}
\indent \textbf{Step 4:} Update tap-weight vector:
\begin{equation}
  \boldsymbol{w}(n)= \boldsymbol{w}(n-1)+\mu\boldsymbol{u}(n)e(n)
\end{equation}
End n loop\\
\textbf{Step 5:} Add squared error for MSE calculation\\
End k loop\\
\textbf{Step 6:} Calculate MSE
\begin{equation}
  \label{eq:MSE}
  MSE=\dfrac{1}{K}\sum_{k=1}^{K}e_k^2(n); \qquad n = 1,2,....N
\end{equation}
\subsection{Effect of Eigenvalue Spread}
The LMS algorithm was run with total K = 500 iterations and the error for each value of the tap-input vector N = 600 was averaged.
This MSE is calculated using the method described above. This is known as the learning curve and it is used to judge the
performance of each experiment. The results for the eigenvalue spread experiment are shown in Figure~\ref{fig:eigenspread}.
The experiment was run with $M = 11, \quad and \quad SNR = 40dB$.

\begin{figure}[H]
  \centering
  \captionsetup{justification=centering,font = small}
  \includegraphics[width=0.55\textwidth, right] {Plots/Project1_Part1_2.jpg}
  \caption{Learning Curves for Different Channels}
    \label{fig:eigenspread}
\end{figure}
These results show that the LMS is very susceptible to eigenvalue spread. Each channel,
which has progressively more distortion, has an increased convergence time and the mean square
error settles at a higher value. It is shown that the LMS is not capable of
removing the distortion from channel four.

\subsection{Effect of Filter Order}
\begin{figure}[H]
  \centering
  \captionsetup{justification=centering,font = small}
  \includegraphics[width=0.55\textwidth, right] {Plots/Project1_Part3.jpg}
  \caption{Channel 2 Learning Curves with Varying Filter Order}
    \label{fig:filterorder1}
\end{figure}
Figure ~\ref{fig:filterorder1} reinforces the notion that the misadjustment
of the LMS filter increases as the filter order increases \cite{goldenberg1985digital}. The parameters for this run were $M = 9, 11, 21 \quad and \quad SNR = 40dB
\quad \mu = 0.075 \quad and \quad N = 600$. While the speed of convergence
generally decreases with increased filter order \cite{goldenberg1985digital}, it typically increases the eigenvalue spread
because the input autocorrelation matrix, $\boldsymbol{R}$, has larger dimensions. Therefore it becomes difficult to
choose a step-size, $\mu$, that will converge for a large filter order.
$$0 < \mu < \dfrac{2}{tr[\boldsymbol{R}]} = \mu < \dfrac{2}{Mr(0)}  = \dfrac{2}{M(r(0) = h_1^2 + h_2^2 + h_3^2 + \sigma^2_v)} $$
For the second channel and a filter order of 21
$\mu \approx 0.082$. The value of 0.075 is very close to this limit and we can see the divergence as expected.
The problem can be fixed by changing the
step size parameter to a more suitable value such as $ \mu = 0.0375$.
%\vspace{-5mm}
\subsection{Analysis of Step Size Parameter}
\vspace{-5mm}
\begin{figure}[H]
  \centering
  \captionsetup{justification=centering,font = small}
  \includegraphics[width=0.55\textwidth, inner] {Plots/Project1_Part4.jpg}
  \caption{Learning Curves of Varying Step Size Parameters}
    \label{fig:stepsize}
\end{figure}
As previously discussed, increasing the step-size parameter will decrease the
convergence time. This is demonstrated by the convergence speed formula:
$$ \tau = \dfrac{1}{\mu\lambda}$$ There are a couple things to keep in mind. First, a
step-size which is smaller than the maximum value is required. This must meet the condtions outlined in the previous section.
\\
\\
However in Figure ~\ref{fig:stepsize} we can see that the settling MSE is higher for the faster convergence. Therefore the learning curve
clearly demostrates the important consideration when selecting step-size; an increased step-size provides
faster convergence at the cost of increased misadjustment.
\subsection{Comparison of Normalized LMS and Standard LMS}
The final experiment studies the performance of the Standard LMS against the Normalized LMS. Two instances of the
Standard LMS with values of $\mu = 0.025, 0.075$ are tested alongside the Normalized LMS. The normalized algorithm has the
benefit of having a time-varying step size:
$$\mu(n) = \dfrac{1}{\boldsymbol{u}^T(n)\boldsymbol{u}(n)}$$
This solves stability issues with certain applications \cite{haykin2008adaptive}. The drawback being that you can no longer
optimize for convergence speed. This concept is demonstrated in Figure ~\ref{fig:normalizedlms}
\begin{figure}[H]
  \centering
  \captionsetup{justification=centering,font = small}
  \includegraphics[width=0.50\textwidth, right] {Plots/Project1_Part5.jpg}
  \caption{Learning Curves of Standard and Normalized LMS Algorithms}
    \label{fig:normalizedlms}
\end{figure}

% needed in second column of first page if using \IEEEpubid
%\IEEEpubidadjcol

\section{Project 2: Recursive Least Squares Algorithm}
The RLS algorithm minimizes the weighted mean square error cost function:
\begin{align*}
  J(n)=\sum_{i=1}^{n}\lambda^{n-i}d^2(i)-2\boldsymbol{w}^T(n)\sum_{i=1}^{n}\lambda^{n-i}\boldsymbol{d}{u}(i)d(i)+ \\ \boldsymbol{w}^T(n)\sum_{i=1}^{n}\lambda^{n-i}\boldsymbol{u}(i)\boldsymbol{u}^T(i)\boldsymbol{w}(i)
\end{align*}
\begin{align*}
  J(n)= \boldsymbol{D} - 2\boldsymbol{w}^T(n)\boldsymbol{z}(n)+\boldsymbol{w}^T(n)\boldsymbol{F}(n)\boldsymbol{w}(n)
\end{align*}
where D is the deterministic variance, \textbf{z} is the deterministic cross-corrleation and \textbf{F}
is the deterministic autocorrelation of the tap-input vector. It is important to note that throughout these projects $\lambda$,
which is known as the forgetting factor,\textbf{is always set to 1} because the statistics are stationary. To avoid calculating the MSE using the brute force
approach, the RLS takes advantage of the Matrix Inversion Lemma to create a method of resursion for updating the inversion of the correlation matrix.
The summary of the implemented RLS algorithm is as follows:\\
\\
\textbf{Step 1:} \\ \vspace{-5mm}
\begin{center} Initialize the weight vector and the inverse correlation matrix\end{center}
$$\boldsymbol{w}(0) = \boldsymbol{0},  \boldsymbol{P}(0) = \delta^{-1}\boldsymbol{I}$$\\
\vspace{-7mm}\\\textbf{Step 2:}\\
For $k = 1:K$ and,\\
For $n = 1:N$\\
(i.e. for each value of the input sequence $u(n), n = 1,2,...N$)
\\ compute the Kalman gain vector
\begin{equation}
  \boldsymbol{K}(n)= \dfrac{\boldsymbol{P}(n-1)\boldsymbol{u}(n)}{\lambda+\boldsymbol{u}^T(n)\boldsymbol{P}(n-1)\boldsymbol{u}(n)}
\end{equation}
\indent \textbf{Step 3:} Compute the a priori error and update the weight vector:
\begin{equation}
  \alpha(n) = d(n) - \boldsymbol{u}(n)\boldsymbol{w}(n-1)
\end{equation}
\begin{equation}
  \boldsymbol{w}(n)= \boldsymbol{w}(n-1)+\boldsymbol{K}(n)\alpha(n)
\end{equation}
End n loop\\
\indent \textbf{Step 4:} Update the inverse correlation matrix
\begin{equation}
  \boldsymbol{P}(n)= \lambda^{-1}[\boldsymbol{P}(n-1)-\boldsymbol{K}(n)\boldsymbol{u}^T(n)\boldsymbol{P}(n-1)]
\end{equation}
\textbf{Step 5:} Add squared error for MSE calculation\\
End k loop\\
\textbf{Step 6:} Calculate MSE
\begin{equation}
  \label{eq:MSE}
  MSE=\dfrac{1}{K}\sum_{k=1}^{K}e_k^2(n); \qquad n = 1,2,....N
\end{equation}
\subsection{Effect of Eigenvalue Spread}
The same experiment that was performed in the LMS project is rerun to observe the RLS' ability to
converge across increasing eigenvalue spread. As is shown in Figure ~\ref{fig:eigenspread2},
the RLS algorithim's convergence speed is not effected by eigenvalue spread. All four channels converge
within roughly 25 iterations of the N loop. This is in stark contrast to the LMS algorithm which
required over 100 iterations to converge even with a large step size. However, the misadjustment is still dependent on
the level of dispersion in the channel as the mean square error settles at a larger value for each consecutive channel. The experiment was run
with $M = 11, \quad \delta = 0.01 \quad SNR = 40dB \quad N = 600$ where, $\delta$ is a parameter for implementation
to avoid division by zero errors.
\begin{figure}[H]
  \centering
  \captionsetup{justification=centering,font = small}
  \includegraphics[width=0.55\textwidth, inner] {Plots/Project2_eigenspread.jpg}
  \caption{Effect of Eigenvalue Spread}
    \label{fig:eigenspread2}
\end{figure}
\subsection{Effect of Filter Order}
Now that we've learned that the RLS convergence is robust against eigenvalue spread
and that the step size parameter is non-existent, we would expect the algorithm to
converge even with a high filter order. For comparison with the LMS we now run the experiment with
$M = 9, 11, 21 \quad \delta = 0.01 \quad SNR = 40dB \quad N = 600$. This is demonstrated in Figure ~\ref{fig:filterorder2}.
As opposed to the LMS algorithm which diverged for $ M = 21$, the RLS is able to track the signal. The plot is offset because
a larger delay is required for a larger filter order so it takes time for it to propagate through the system. Figure ~\ref{fig:filterorder2}
proves our theory for convergence even with a high filter order.
\begin{figure}[H]
  \centering
  \captionsetup{justification=centering,font = small}
  \includegraphics[width=0.55\textwidth, inner] {Plots/Project2_filterorder.jpg}
  \caption{Effect of Filter Order}
    \label{fig:filterorder2}
\end{figure}

\subsection{Tap-Weight Analysis}
Next we take a look at the tap-weight vectors for the steady-stae averaged RLS algorithm. Figure ~\ref{fig:weights} is a stem plot that shows all
eleven tap values. The symmetry is shown around the center tap, $W_6$. This is intuitive when considering that the channel has even symmetry. $W_6$ is plotted in Figure
~\ref{fig:w6} and it is seen that it reaches a steady-state value
within 10 samples. The settled value is approximately 1.15.
\begin{figure}[H]
  \centering
  \captionsetup{justification=centering,font = small}
  \includegraphics[width=0.55\textwidth, inner] {Plots/Project2_weights.jpg}
  \caption{Stem Plot of Steady-State Tap-Weights}
    \label{fig:weights}
\end{figure}
\begin{figure}[H]
  \centering
  \captionsetup{justification=centering,font = small}
  \includegraphics[width=0.55\textwidth, inner] {Plots/Project2_w6.jpg}
  \caption{Center Tap Weight, $w_6$}
    \label{fig:w6}
\end{figure}
The magnitude spectra of Channel 1's response, the adaptive filter, and the resulting equalized output
are all plotted in Figure ~\ref{fig:magspec}. As is expected, the output of the filter is completely equalized and shows a flat response across
frequency.
\begin{figure}[H]
  \centering
  \captionsetup{justification=centering,font = small}
  \includegraphics[width=0.55\textwidth, inner] {Plots/Project2_MagnitudeSpectrum.jpg}
  \caption{Magnitdue Spectra of Equalizer}
    \label{fig:magspec}
\end{figure}
\subsection{Small SNR Analysis}
Finally a comparison between the RLS and LMS with small $SNR = 10dB$ is shown in Figure ~\ref{fig:smallsnr}. Neither the RLS or LMS performs nearly as well
as when the larger signal-to-noise ratio of $40dB$ was used. Their settling values for MSE are roughly 0.1 and 0.8 respectively as opposed to the previous values of roughly $10^{-4}$.
This experiment gives us a good benchmark for understanding the constraints of the algorithm in a real world implementation.
\begin{figure}[H]
  \centering
  \captionsetup{justification=centering,font = small}
  \includegraphics[width=0.55\textwidth, inner] {Plots/Project2_smallSNR.jpg}
  \caption{RLS vs LMS Comparison using $SNR = 10dB$}
    \label{fig:smallsnr}
\end{figure}
\section{Project 3: Recursive Least Squares Lattice}
The Recursive Least Squares Lattice is designed to solve the minimization problem
using a lattice structure for the prediction phase and an estimator. The lattice predictor
is a pipelined structure. The forward and backward reflection coefficients are used to minimize
the prediction error powers:
\begin{equation}
F_m(n)=\sum_{i=1}^{n}\lambda^{n-i}f^2_m(i); \qquad n = 1,2,....N
\end{equation}
\begin{equation}
B_m(n)=\sum_{i=1}^{n}\lambda^{n-i}b^2_m(i); \qquad n = 1,2,....N
\end{equation}
Joint Process Esimation has a lattice structure and an estimation phase using regression coefficients. This structure is
shown in Figure ~\ref{fig:jointprocess}.
\begin{figure*}
  \centering
  \captionsetup{justification=centering,font = small}
  \includegraphics[width=0.75\textwidth] {Plots/jointprocess}
  \caption{Block Diagram for Joint Process Estimation \cite{lecturenotes}}
    \label{fig:jointprocess}
\end{figure*}
It is important to note that in Joint Process Estimation the prediction step starts at $ m = 1 $ and the estimation starts
at $m = 0$. These are adjusted to 1, and 2 for Matlab indexing purposes. Prediction and estimation are combined into one stage for implementation
and a simple if statement is used to begin the processes at different indices.
The summary of the implemented RLSL algorithm is as follows:\\
\\
\textbf{Step 1:} Initialize the algorithm for both prediction and estimation:
$b_m(n) = 0, \quad B_m(n) = \delta, \quad F_m(n) = \delta, \quad \Delta_{m-1}(n) = 0, \quad \gamma_{m-1}(n) = 1,\quad \rho_{m}(0) = 0$\\
\textbf{Step 2:}\\
For $k = 1:K$\\
and,\\
For $n = 0:N$\\
(i.e. for each value of the input sequence $u(n), n = 1,2,...N,$)
\\ \indent Start the algorithm and initialize the required parameters for the input stage.
$$b_0(n) = u(n), \quad f_0(n) = u(n), \quad F_0(n) = \lambda F_0(n-1) + u^2(n), \quad$$
$$B_0(n) = F_0(n), \quad \gamma_0(n) = 1,\quad e_0(n) = d(n)$$
\indent \textbf{Step 3:} Time update for each predictor stage
\begin{equation}
  \Delta_{m-1}(n) = \Delta_{m-1}(n) + \dfrac{b_{m-1}(n-1)f_{m-1}(n)}{\gamma_{m-1}(n-1)}
\end{equation}
\begin{equation}
  \gamma_{f,m}(n)= -\dfrac{\Delta_{m-1}(n-1)}{B_{m-1}(n-1)}
\end{equation}
\begin{equation}
  \gamma_{b,m}(n)= -\dfrac{\Delta_{m-1}(n-1)}{F_{m-1}(n)}
\end{equation}
\begin{equation}
  \rho_m(n) = \lambda\rho_m(n-1)+\dfrac{b_{m}(n)}{\gamma_{m}(n)}\alpha_m(n)
\end{equation}
\begin{equation}
  \kappa_m(n) = \dfrac{\rho_{m}(n)}{B_{m}(n)}
\end{equation}
\indent Order update:
\begin{equation}
  f_m(n) = f_{m-1}(n) + \Gamma_{f,m}(n)b_{m-1}(n-1)
\end{equation}
\begin{equation}
  b_m(n) = b_{m-1}(n-1) + \Gamma_{b,m}(n)f_{m-1}(n)
\end{equation}
\begin{equation}
  F_m(n) = F_{m-1}(n) + \Gamma_{f,m}(n)\Delta_{m-1}(n)
\end{equation}
\begin{equation}
  B_m(n) = B_{m-1}(n-1) + \Gamma_{b,m}(n)\Delta_{m-1}(n)
\end{equation}
\begin{equation}
  \gamma_m(n) = \gamma_{m-1}(n) - \dfrac{b^2_{m-1}(n)}{B_{m-1}(n)}
\end{equation}
\begin{equation}
  \alpha_{m+1}(n) = \alpha_{m}(n) - \kappa_m(n)b_m(n)
\end{equation}
End n loop\\
\indent \textbf{Step 4:} calculate a posteriori error for final stage before squaring error
\begin{equation}
  e_{11}(n) = \dfrac{\alpha_{11}(n)}{\gamma_{11}(n)}
\end{equation}
\textbf{Step 5:} Add squared error for MSE calculation\\
End k loop\\
\textbf{Step 6:} Calculate MSE
\begin{equation}
  \label{eq:MSE}
  MSE=\dfrac{1}{K}\sum_{k=1}^{K}e_k^2(n); \qquad n = 1,2,....N
\end{equation}
\subsection{Effect of Eigenvalue Spread}
The convergence behaviour of the RLSL is very similar to the RLS. Both algorithms converge in roughly the
same amount of time steps and the RLSL is also able to equalize the fourth channel. The RLSL does seem to have a
slight performance advantage over the RLS as the MSE is marginally lower. However, considering the complexity of the implementation
and the hardware resource that would be required in real-time it is not practical to use this algorithm. In \cite{satorius1981application}, the potential benefits of using
the RLSL method, such as insensitivity to quantization errors, is presented.
\begin{figure}[H]
  \centering
  \captionsetup{justification=centering,font = small}
  \includegraphics[width=0.55\textwidth, inner] {Plots/Project3_MSEE.jpg}
  \caption{A Posteriori Mean Square Erro}
    \label{fig:MSEEp}
\end{figure}
\subsection{Reflection Coefficients}
The reflection coefficients are plotted in Figures ~\ref{fig:gammaf} and ~\ref{fig:gammab}. The
forward parameter has a much larger spike at the beginning of the algorithm than the backward does. The backward
coefficient never is larger in magnitude than 0.03. It can be seen that both forward and backward coefficients converge to
almost 0 within a couple hundred iterations. It can be seen that neither magnitude value is greather than 1 and therefore
the analysis is corrected.
\begin{figure}[H]
  \centering
  \captionsetup{justification=centering,font = small}
  \includegraphics[width=0.55\textwidth, inner] {Plots/Project3_GAMMA_F.jpg}
  \caption{Forward Reflection Coefficient}
    \label{fig:gammaf}
\end{figure}
\begin{figure}[H]
  \centering
  \captionsetup{justification=centering,font = small}
  \includegraphics[width=0.55\textwidth, inner] {Plots/Project3_GAMMA_B.jpg}
  \caption{Backward Reflection Coefficient}
    \label{fig:gammab}
\end{figure}
\subsection{Likelihood Parameter}
The likilhood parameter indicates stationarity within a dataset. Since this data is
stationary we expect to see one instance where $\gamma$ drops and then an exponential
increase back to a congerging value of 1. This means that the a posteriori error approaches the a priori error.
This is confirmed by Figure ~\ref{fig:likelihood}.
\begin{figure}[H]
  \centering
  \captionsetup{justification=centering,font = small}
  \includegraphics[width=0.55\textwidth, inner] {Plots/Project3_LIKLIHOOD.jpg}
  \caption{Likelihood Parameter, $\gamma_m$}
    \label{fig:likelihood}
\end{figure}
\subsection{Regression Coefficients}
The regression coefficients for the implemented RLSL algorithm are shown in Figure ~\ref{fig:regcoeff}.
These coefficients are used to estimate the output by performing a regression sum, that is:
\begin{equation}
  \label{eq:regsum}
  \hat{d}(n)= y(n) = \sum_{i=0}^{M}k_ib_i(n);
\end{equation}
These coefficients play a similar role as the tap-weight coefficients in a Wiener filter. However, as seen in
the plot, these coefficients do not have a perfect symmetry around the center tap.
\begin{figure}[H]
  \centering
  \captionsetup{justification=centering,font = small}
  \includegraphics[width=0.55\textwidth, inner] {Plots/Project3_STEM.jpg}
  \caption{Regression Coefficients}
    \label{fig:regcoeff}
\end{figure}

% An example of a floating figure using the graphicx package.
% Note that \label must occur AFTER (or within) \caption.
% For figures, \caption should occur after the \includegraphics.
% Note that IEEEtran v1.7 and later has special internal code that
% is designed to preserve the operation of \label within \caption
% even when the captionsoff option is in effect. However, because
% of issues like this, it may be the safest practice to put all your
% \label just after \caption rather than within \caption{}.
%
% Reminder: the "draftcls" or "draftclsnofoot", not "draft", class
% option should be used if it is desired that the figures are to be
% displayed while in draft mode.
%
%\begin{figure}[!t]
%\centering
%\includegraphics[width=2.5in]{myfigure}
% where an .eps filename suffix will be assumed under latex,
% and a .pdf suffix will be assumed for pdflatex; or what has been declared
% via \DeclareGraphicsExtensions.
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure}

% Note that the IEEE typically puts floats only at the top, even when this
% results in a large percentage of a column being occupied by floats.


% An example of a double column floating figure using two subfigures.
% (The subfig.sty package must be loaded for this to work.)
% The subfigure \label commands are set within each subfloat command,
% and the \label for the overall figure must come after \caption.
% \hfil is used as a separator to get equal spacing.
% Watch out that the combined width of all the subfigures on a
% line do not exceed the text width or a line break will occur.
%
%\begin{figure*}[!t]
%\centering
%\subfloat[Case I]{\includegraphics[width=2.5in]{box}%
%\label{fig_first_case}}
%\hfil
%\subfloat[Case II]{\includegraphics[width=2.5in]{box}%
%\label{fig_second_case}}
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure*}
%
% Note that often IEEE papers with subfigures do not employ subfigure
% captions (using the optional argument to \subfloat[]), but instead will
% reference/describe all of them (a), (b), etc., within the main caption.
% Be aware that for subfig.sty to generate the (a), (b), etc., subfigure
% labels, the optional argument to \subfloat must be present. If a
% subcaption is not desired, just leave its contents blank,
% e.g., \subfloat[].


% An example of a floating table. Note that, for IEEE style tables, the
% \caption command should come BEFORE the table and, given that table
% captions serve much like titles, are usually capitalized except for words
% such as a, an, and, as, at, but, by, for, in, nor, of, on, or, the, to
% and up, which are usually not capitalized unless they are the first or
% last word of the caption. Table text will default to \footnotesize as
% the IEEE normally uses this smaller font for tables.
% The \label must come after \caption as always.
%
%\begin{table}[!t]
%% increase table row spacing, adjust to taste
%\renewcommand{\arraystretch}{1.3}
% if using array.sty, it might be a good idea to tweak the value of
% \extrarowheight as needed to properly center the text within the cells
%\caption{An Example of a Table}
%\label{table_example}
%\centering
%% Some packages, such as MDW tools, offer better commands for making tables
%% than the plain LaTeX2e tabular which is used here.
%\begin{tabular}{|c||c|}
%\hline
%One & Two\\
%\hline
%Three & Four\\
%\hline
%\end{tabular}
%\end{table}


% Note that the IEEE does not put floats in the very first column
% - or typically anywhere on the first page for that matter. Also,
% in-text middle ("here") positioning is typically not used, but it
% is allowed and encouraged for Computer Society conferences (but
% not Computer Society journals). Most IEEE journals/conferences use
% top floats exclusively.
% Note that, LaTeX2e, unlike IEEE journals/conferences, places
% footnotes above bottom floats. This can be corrected via the
% \fnbelowfloat command of the stfloats package.


\section{Conclusion}
The experiments conducted for equalization of four dispersive channels in ENEL 671
has brought great insight into implementation techniques, performance, design considerations
and applicability of three different adaptive algorithms. The LMS algorithm is shown to be
the most rudimentary algorithm. The advantage of this is its relatively simple implementation. However,
careful consideration of it's limitations such as the step-size, filter order, convergence speed and their interdependence must
be carefully analyzed. The Recursive Least Squares algorithm is very robust to channel distortion (eigenvalue spread) and the
algorithm converges very quickly. This method is computationaly more complex than the LMS, therefore depending
on the requirements of the application it may not be required. The RLSL algorithm is by far the most complicated to implement.
Careful analysis of parameters such as the likelihood factor need to be monitored to understand the performance. The mean square
error of the RLSL was very marginally improved from the RLS with a very similar convergence speed. However, since a lattice
structure is highly pipelined the implementation in VLSI is very practicle.


% if have a single appendix:
%\appendix[Proof of the Zonklar Equations]
% or
%\appendix  % for no appendix heading
% do not use \section anymore after \appendix, only \section*
% is possibly needed

% use appendices with more than one appendix
% then use \section to start each appendix
% you must declare a \section before using any
% \subsection or using \label (\appendices by itself
% starts a section numbered zero.)
%

%\appendices
%\section{Proof of the First Zonklar Equation}
%Appendix one text goes here.

% you can choose not to have a title for an appendix
% if you want by leaving the argument blank
%\section{}
%Appendix two text goes here.



% Can use something like this to put references on a page
% by themselves when using endfloat and the captionsoff option.
\ifCLASSOPTIONcaptionsoff
  \newpage
\fi



% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://mirror.ctan.org/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)
%\begin{thebibliography}{1}
%\cite{iso}

\section{References}
\printbibliography[heading=none]
%\bibitem{IEEEhowto:kopka}
%A.~Sesay, \emph{Adaptive Signal Processing}, ENEL 671Lecture Notes. \hskip 1em plus
%  0.5em minus 0.4em\relax Department of Electrical \& Computer Engineering, University of Calgary, 2016.
\normalsize
%\end{thebibliography}

% biography section
%
% If you have an EPS/PDF photo (graphicx package needed) extra braces are
% needed around the contents of the optional argument to biography to prevent
% the LaTeX parser from getting confused when it sees the complicated
% \includegraphics command within an optional argument. (You could create
% your own custom macro containing the \includegraphics command to make things
% simpler here.)
%\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{mshell}}]{Michael Shell}
% or if you just want to reserve a space for a photo:



% You can push biographies down or up by placing
% a \vfill before or after them. The appropriate
% use of \vfill depends on what kind of text is
% on the last page and whether or not the columns
% are being equalized.

%\vfill

% Can be used to pull up biographies so that the bottom of the last one
% is flush with the other column.
%\enlargethispage{-5in}



% that's all folks
\end{document}
